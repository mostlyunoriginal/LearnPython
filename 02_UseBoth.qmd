---
title: "Recreate Some dplyr Shiz with Python and Polars"
format: html
---

```{r}
#| echo: false
#| output: false

#library(tidyverse)
```


## 1. Basic Summarize without Generalization
### r version
```{r}
library(dplyr)

table<-mtcars %>%
    group_by(cyl) %>%
    summarize(mpg.mean=mean(mpg))

print(table)

```

### python version
I'm using polars instead of pandas as it feels more natural to me coming from tidy-style R programming. I'm invoking lazy evaluation (as opposed to eager) by using a lazy frame. This is actually more similar to dtplyr than dplyr. A query plan is assembled as `q` and then evaluated and the result saved to `table`. This 'lazy' approach allows polars to do query optimization.

While there is no pipe, the method chaining approach used below *feels* very familiar.
```{python}
import polars as pl

mtcars=pl.DataFrame(r.mtcars).lazy()

q=(
    mtcars
    .group_by(pl.col("cyl"))
    .agg(pl.col("mpg").mean().alias("mpg.mean"))
)

table=q.collect()

print(table)

```

## 2. Basic Mutate with Grouping and without Generalization
### r version
In R I can create a recode via `mutate()` that utilizes both group-level statistics and record-level data. This can be done in a single step with very little code.
```{r}
table<-mtcars %>%
    group_by(cyl) %>%
    mutate(rel.mpg=mpg/mean(mpg))

print(table)

```

### python version
With polars, it doesn't appear that I can mix the `group_by` and `with_columns` contexts. Equivalent output necessitates a 2-step approach that calculates and joins on the the group-level statistics before invoking my recode expression in the `with_columns` context, which is roughly analogous to `dplyr::mutate()`.

Note that I won't be surprised to learn that what I did above in the dplyr version *is* possible with polars (spoiler: it is--see update below) and I just don't yet know how. In any case, the 2-step works and still fits within a single method chain without too much additional typing.

Update: while `group_by` and `with_columns` contexts can't be mixed, grouping can be incorporated into the `with_columns` context via the window function `over()`. This is actually *more* powerful than `group_by() %>% mutate()` in R because you can have a different groupings across expressions in a common `with_columns` context.
```{python}
q=(
    mtcars.join(
        mtcars
        .group_by("cyl")
        .agg(pl.col("mpg").mean().alias("den"))
        ,on="cyl"
        ,how="left"
    )
    .with_columns((pl.col("mpg")/pl.col("den")).alias("rel.mpg"))
    .drop("den")
)

table=q.collect()

print(table)

#this produces equivalent output, while being computationally optimal and more concise
q=(
    mtcars
    .with_columns(
        ((pl.col("mpg"))/(pl.col("mpg").mean().over("cyl"))).alias("rel.mpg")
    )
)

table=q.collect()

print(table)

```

## 3. Summarize Generalized by Variable Type with Across()
### r version
```{r}
mtcars %>%
    group_by(cyl,gear) %>%
    summarize(
        across(
            .cols=where(is.double)
            ,.fns=mean
            ,.names="{.col}_mean"
        )
    )

```

### python version
Note that there are *many* selector functions available, as explained [here](https://docs.pola.rs/api/python/stable/reference/selectors.html). This is a good example of how a selector function (`cs.float()` in this case) works in conjunction with namaing methods, like `expr.name.suffix()` below. See other methods [here](https://docs.pola.rs/api/python/stable/reference/expressions/name.html).
```{python}
import polars.selectors as cs

q=(
    mtcars
    .group_by("cyl","gear")
    .agg(cs.float().mean().name.suffix("_mean"))
)

table=q.collect()

print(table)

```

## 4. Function for n & pct by Grouping Variables
In both cases I want a custom function to create simple, list-style frequency tables based on one or more variables provided by the user.

### r version
I use dynamic dots here to tunnel in the variables provided by the user in the function call. This is powerful and flexible, allowing for 0+ variables provided as naked symbols rather than strings (`doit()`); an alternative version (`doit2()`) also uses dynamic dots, but with the intention to call with variable names provided as strings--this scales up better and is more comparable to the python version.
```{r}
library(rlang)

doit<-function(df,...){
  df %>%
    ungroup() %>%
    mutate(N=n()) %>%
    group_by(...) %>%
    summarize(n=n(),pct=n()*100/mean(N),.groups="drop") %>%
    mutate(cumn=cumsum(n),cumpct=cumsum(pct))
}

doit(mtcars)
doit(mtcars,cyl)
doit(mtcars,cyl,gear)

doit2<-function(df,...){
    vars<-dots_list(...) %>%
        list_c() %>%
        syms()

    df %>%
        ungroup() %>%
        mutate(N=n()) %>%
        group_by(!!!vars) %>%
        summarize(n=n(),pct=n()*100/mean(N),.groups="drop") %>%
        mutate(cumn=cumsum(n),cumpct=cumsum(pct))
}

doit2(mtcars)
doit2(mtcars,"cyl")
doit2(mtcars,"cyl","gear")

```

### python version
The version below gets very close! The only differences are that the python version of `doit()` doesn't work as-is if 0 variables are provided (though it could be modifed to only conditionally invoke the `group_by` context) and the variable names are passed as strings (i.e., this doesn't seem to be optional as with the tidy versions). This latter point should actually simplify some situations that are complex due to data mask ambiguities in tidy functions.
```{python}
def doit(df,*argv):
    q=(
        df
        .with_columns(pl.len().alias("N"))
        .group_by(*argv)
        .agg(
            pl.len().alias("n")
            ,((pl.len()*100)/pl.col("N").mean()).alias("pct")
        )
        .sort(*argv)
        .with_columns(
            pl.col("n").cum_sum().alias("cumn")
            ,pl.col("pct").cum_sum().alias("cumpct")
        )
    )
    table=q.collect()
    print(table)

doit(mtcars,"cyl")
doit(mtcars,"cyl","gear")

```

## 5. Iterate a Custom Function
Here I want to apply the `doit` functions over parameters.

### r version
I use `purrr::pmap()` in the R version with a list of parameters. Since I defined the R version of `doit` to take naked symbols, the mapped version is kind of clunky--a cleaner alternative based on `doit2` highlights the point.
```{r}
parms<-list(
    list(mtcars,mtcars)
    ,"var1"=list(mtcars$cyl,mtcars$cyl)
    ,"var2"=list(mtcars$gear,mtcars$am)
)

purrr::pmap(parms,doit)

parms2<-list(
    c("cyl","cyl")
    ,c("gear","am")
)

purrr::pmap(parms2,doit2,df=mtcars)

```

### python version
Super simple! I combine 3 parameter series into a single iterator object via `zip`--can then map `doit` over `parms` via `itertools.starmap`. Awesome!
```{python}
import itertools as it

parms=zip(
    [mtcars,mtcars]
    ,['cyl','cyl']
    ,['gear','am']
)

list(it.starmap(doit,parms))

```

## 6. Conditional Recode

### r version
```{r}
mtcars %>%
    mutate(
        mpg.cat=case_when(
            mpg<10~"very bad"
            ,mpg<15~"bad"
            ,mpg<20~"okay"
            ,mpg<25~"good"
            ,TRUE~"great"
        )
    ) %>%
    arrange(desc(mpg))

```

### python version
```{python}
q=(
    mtcars
    .with_columns(
        pl.when(pl.col("mpg")<10).then(pl.lit("very bad"))
        .when(pl.col("mpg")<15).then(pl.lit("bad"))
        .when(pl.col("mpg")<20).then(pl.lit("okay"))
        .when(pl.col("mpg")<25).then(pl.lit("good"))
        .otherwise(pl.lit("great"))
        .alias("mpg.cat")
    )
    .sort("mpg",descending=True)
)

df=q.collect()

print(df)

```