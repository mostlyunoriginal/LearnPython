---
title: "Recreating Some Tidy-Style R Operations with Python and Polars"
date: 2025-03-14
format: 
    html:
        theme: journal
author:
    - name: G. Lance Couzens
      email: gcouzens@rti.org  
abstract: >
    In this document I provide several examples that demonstrate how to do most of the basic data manipulation operations, and some scalable programming approaches, as equivalently as possible in both Tidy-style R and Polars-style Python.

    For tidyverse syntax, refer to the `Reference` page for each package under <https://www.tidyverse.org/>. For Polars, consult the [user guide](https://docs.pola.rs/) for high-level information, and dive into [the API reference](https://docs.pola.rs/api/python/dev/reference/index.html) for full detail and syntax.
---

## Background

Before diving into the examples below, it's important to acknowledge some general differences between R and Python and specific differences in style and approach between Tidy- and Polars-style data manipulation. First and foremost: Python has a strong object orientation while R is essentially a functional language. The practical impact of that difference here is that Python objects are manipulated, or their attributes extracted, by way of methods, while R objects are inputs and outputs of functions. But Python uses functions too, and in fact methods are themselves functions, so this can be very confusing!

What is a method, then? In simple terms, it's a function defined as part of the blueprint for a given type (or 'class') of object. A Polars DataFrame is a class of object, and there are certain functions defined in that class---these are the Polars DataFrame methods. By creating a *specific* DataFrame, we 'instantiate' the class into an object, and we can deploy [a predefined set of methods](https://docs.pola.rs/api/python/dev/reference/dataframe/index.html) to do things with or to that object.

In both R and Python we often want to do several operations in a row without distinct assignments for each intermediate step of a process. In R---and especially in the Tidy style of R programming---we can use piping with either the `magrittr` or base pipes (`%>%` and `|>`, respectively) to achieve this. The resulting pipeline starts with an object, passes that object into a function which returns a new object which is passed into another function, and so on and so forth until the desired object is returned by the final function in the pipeline and is captured with an assignment, returned to the console, or passed as input to another pipeline or function. Consider the following example.

```{r}
#| output: false
#| echo: false

library(dplyr)
```
```{r}
#| output: false

cyls<-mtcars %>% #1, 5
  distinct(cyl) %>% #2
  arrange(cyl) %>% #3
  pull() #4

```

Here, we start with the data frame `mtcars` (1), which is piped as input to the `distinct()` function along with the column reference `cyl` (2), which returns a data frame containing only the column `cyl` and one row for each distinct value. This is piped as input to `arrange()` (3) along with a column reference to `cyl`, which returns a sorted data frame. This is piped into `pull()` (4), which extracts a single column (the only one there: `cyl`) as a vector. This final object is then assigned to the environment variable `cyls` (5). Now consider the Python version which utilizes a technique called 'method chaining'.

```{python}
#| echo: false
#| output: false

import polars as pl

mtcars=pl.read_csv("mtcars.csv")

```

```{python}
#| output: false

cyls=( #5
    mtcars #1
    .unique("cyl") #2
    .sort("cyl") #3
    .get_column("cyl") #4
)

```

Here, we start with `mtcars`, a Polars DataFrame (1). We then apply the `unique()` method with a reference to the column `cyl` (2), yielding a Polars DataFrame containing the distinct values of `cyl` (note that it still contains all the other variables too!). Calling the `sort()` method sorts the rows by the values of `cyl` (3). The Polars DataFrame method `get_column()` (4) extracts a single column and yields a Polars Series (analogous to the atomic vectors that comprise R data frame columns). The resulting Series is assigned to the variable `cyls` (5).

Both of these code blocks look quite similar, and the Python version should feel familiar to anyone who employs the Tidy-style of programming in R. Now that we've seen method chaining in action we can introduce a twist that unlocks some additional efficiency and that may seem strange compared to the Tidy style. The Python block above utilizes what's called 'eager evaluation', which means the code inside `cyls=(...)` is immediately evaluated and in exactly the manner we have specified. However, Polars is actually implemented in Rust (a high performance systems programming language) and has a query optimization capability that we can exploit via something called 'lazy evaluation'. The following 'lazy' alternative to the previous example gathers our instructions, performs query optimization (yielding a 'query plan'), and ultimately executes an optimized query only when we invoke the `collect()` method (a method of Polars LazyFrames which result from invoking the `lazy()` method of a regular Polars DataFrame).

```{python}
#| output: false

cyls=(
    mtcars
    .lazy()
    .unique("cyl")
    .sort("cyl")
    .collect()
    .get_column("cyl")
)

```

Note that Polars LazyFrames do not have a `get_column()` method like DataFrames do---it can therefore only be invoked *after* collection. The advantages of lazy evaluation in this trivial example would not be noticeable but could be significant depending on the size of the data and the complexity of the query. Lazy evaluation also allows for efficient processing of larger-than-memory data frames. See the [User guide](https://docs.pola.rs/user-guide/lazy/) for more detail. This approach may seem familiar to anyone who has used the `dtplyr` R package which allows the user to proved `dplyr` syntax which is translated into `data.table` (which is written primarily in C and is much faster than `dplyr`) under the hood.

Without further ado, let's dive into some examples.

## 1. Basic Summarize without Generalization across Variables

Here, we take on a very simple and very common task: calculating the mean of a continuous variable (`mpg`) by the levels of a categorical variable (`cyl`).

### R Version

The Tidy approach utilizes a pipeline comprised of the `mtcars` data frame and the `group_by()` and `summarize()` functions. Note that these functions take a data frame (or tibble) as the first argument, but prevailing style allows this to be passed implicitly (as is done here).
```{r}
library(dplyr)

table<-mtcars %>%
    group_by(cyl) %>%
    summarize(mpg.mean=mean(mpg))

print(table)

```

### Python Version

The Polars approach below begins by reading the R `mtcars` data frame into the Polars LazyFrame `mtcars`. The LazyFrame method `group_by()` is invoked followed by the `agg()` method. `agg()` contains an expression that is itself a method chain which yields the mean values for each group as the new variable `mpg.mean`. `table` is a Polars DataFrame realized as the result of evaluating an optimized query plan (via `collect()`).
```{python}
import polars as pl

mtcars=pl.LazyFrame(r.mtcars)

q=(
    mtcars
    .group_by("cyl")
    .agg(pl.col("mpg").mean().alias("mpg.mean"))
)

table=q.collect()

print(table)

```

## 2. Basic Mutate with Grouping and without Generalization

Here we want to add a new variable to our data frame---the new variable is the ratio of each value of `mpg` relative to the mean value for the group (defined by the levels of the variable `cyl`).

### R Version

In R I can create the new variable with a call to `mutate()` that utilizes both group-level statistics and record-level data. This can be done in a single step with very little code.
```{r}
table<-mtcars %>%
    group_by(cyl) %>%
    mutate(rel.mpg=mpg/mean(mpg))

print(table)

```

### Python Version

The Python version uses the `with_columns()` LazyFrame method. Here, unlike in the R version, the grouping is baked into the recode expression itself by way of `over()`. Aside from looking a bit different, the Polars approach is more powerful because each expression can utilize its own grouping. Note that the Polars documentation utilizes a 'contexts' and 'expressions' framework to describe what could also be referred to as methods or method chains. In this example, `with_columns()` is the context in which the expression yielding the new variable `rel.mpg` is nested.
```{python}
q=(
    mtcars
    .with_columns(
        (pl.col("mpg")/pl.col("mpg").mean().over("cyl")).alias("rel.mpg")
    )
)

table=q.collect()

print(table)

```

## 3. Summarize Generalized by Variable Type with Across

### R Version
```{r}
mtcars %>%
    group_by(cyl,gear) %>%
    summarize(
        across(
            .cols=where(is.double)
            ,.fns=mean
            ,.names="{.col}_mean"
        )
    )

```

### Python Version
Note that there are *many* selector functions available, as explained [here](https://docs.pola.rs/api/python/stable/reference/selectors.html). This is a good example of how a selector function (`cs.float()` in this case) works in conjunction with namaing methods, like `expr.name.suffix()` below. See other methods [here](https://docs.pola.rs/api/python/stable/reference/expressions/name.html).
```{python}
import polars.selectors as cs

q=(
    mtcars
    .group_by("cyl","gear")
    .agg(cs.float().mean().name.suffix("_mean"))
)

table=q.collect()

print(table)

```

## 4. Function for n & pct by Grouping Variables
In both cases I want a custom function to create simple, list-style frequency tables based on one or more variables provided by the user.

### R Version
I use dynamic dots (`...`) here to tunnel in the variables provided by the user in the function call. This is powerful and flexible, allowing for 0+ variables provided as naked symbols rather than strings (`doit()`); an alternative version (`doit2()`) also uses dynamic dots, but with the intention to call with variable names provided as strings---this scales up better and is more comparable to the python version.
```{r}
library(rlang)
library(purrr)

doit<-function(df,...){
  df %>%
    ungroup() %>%
    mutate(N=n()) %>%
    group_by(...) %>%
    summarize(n=n(),pct=n()*100/mean(N),.groups="drop") %>%
    mutate(cumn=cumsum(n),cumpct=cumsum(pct))
}

doit(mtcars)
doit(mtcars,cyl)
doit(mtcars,cyl,gear)

doit2<-function(df,...){
    vars<-dots_list(...) %>%
        list_c() %>%
        syms()

    df %>%
        ungroup() %>%
        mutate(N=n()) %>%
        group_by(!!!vars) %>%
        summarize(n=n(),pct=n()*100/mean(N),.groups="drop") %>%
        mutate(cumn=cumsum(n),cumpct=cumsum(pct))
}

doit2(mtcars)
doit2(mtcars,"cyl")
doit2(mtcars,"cyl","gear")

```

### Python Version
The version below gets very close! The only differences are that the python version of `doit()` doesn't work as-is if 0 variables are provided (though it could be modifed to only conditionally invoke the `group_by` context) and the variable names are passed as strings (i.e., this doesn't seem to be optional as with the tidy versions). This latter point should actually simplify some situations that are complex due to data mask ambiguities in tidy functions.
```{python}
def doit(df,*argv):
    q=(
        df
        .with_columns(pl.len().alias("N"))
        .group_by(*argv)
        .agg(
            pl.len().alias("n")
            ,((pl.len()*100)/pl.col("N").mean()).alias("pct")
        )
        .sort(*argv)
        .with_columns(
            pl.col("n").cum_sum().alias("cumn")
            ,pl.col("pct").cum_sum().alias("cumpct")
        )
    )
    table=q.collect()
    print(table)

doit(mtcars,"cyl")
doit(mtcars,"cyl","gear")

```

## 5. Iterate a Custom Function
Here I want to apply the `doit` functions over parameters.

### R Version
I use `purrr::pmap()` in the R version with a list of parameters. Since I defined the R version of `doit` to take naked symbols, the mapped version is kind of clunky---a cleaner alternative based on `doit2` highlights the point.
```{r}
parms<-list(
    list(mtcars,mtcars)
    ,"var1"=list(mtcars$cyl,mtcars$cyl)
    ,"var2"=list(mtcars$gear,mtcars$am)
)

pmap(parms,doit)

parms2<-list(
    c("cyl","cyl")
    ,c("gear","am")
)

pmap(parms2,doit2,df=mtcars)

```

### Python Version
Super simple! I combine 3 parameter lists into a single iterator object via `zip`---I can then map `doit` over `parms` via `itertools.starmap`.
```{python}
import itertools as it

parms=zip(
    [mtcars,mtcars]
    ,['cyl','cyl']
    ,['gear','am']
)

list(it.starmap(doit,parms))

```

## 6. Conditional Recode

### R Version
```{r}
mtcars %>%
    mutate(
        mpg.cat=case_when(
            mpg<10~"very bad"
            ,mpg<15~"bad"
            ,mpg<20~"okay"
            ,mpg<25~"good"
            ,TRUE~"great"
        )
    ) %>%
    arrange(desc(mpg))

```

### Python Version
This is clearly more wordy than the r version above. Note that `pl.lit()` seemed to be necessary for creating the recode as a string (maybe because the column used in the conditional is a float?).
```{python}
q=(
    mtcars
    .with_columns(
        pl.when(pl.col("mpg")<10).then(pl.lit("very bad"))
        .when(pl.col("mpg")<15).then(pl.lit("bad"))
        .when(pl.col("mpg")<20).then(pl.lit("okay"))
        .when(pl.col("mpg")<25).then(pl.lit("good"))
        .otherwise(pl.lit("great"))
        .alias("mpg.cat")
    )
    .sort("mpg",descending=True)
)

df=q.collect()

print(df)

```

## 7. Stack Data Frames by List Binding with Map and Anonymous Function
What I'm achieving with this example---returning `mtcars`---isn't very useful, but it illustrates something I do a lot: mapping an anonymous function over a vector to create a list of data frames which I subsequently stack together via row binding. In other words, in this example I'm reassembling `mtcars` by stacking together portions returned from each iteration of `map`.

### R Version
Pretty straight forward. Note that parms is a vector here.
```{r}
parms<-distinct(mtcars,cyl) %>%
  pull()

list<-map(
  parms
  ,function (x){
    mtcars %>%
      dplyr::filter(cyl==x) %>%
      arrange(desc(mpg))
  }
)

df<-list_rbind(list)

print(df)

```

### Python Version
This is extremely similar. Note that I'm pulling from the csv of `mtcars` to utilize eager evaluation (for simplicity).
```{python}
mtcars=pl.read_csv("mtcars.csv")

parms=mtcars.get_column("cyl").unique()

df=map(
    lambda x: (
        mtcars
        .filter(pl.col("cyl")==x)
        .sort("mpg",descending=True)
    )
    ,parms
)

df=pl.concat(list(df))

print(df)

```

## 8. Stack Data Frames by List Binding with pmap and Anonymous Function of Dots (`...`)
This example generalizes the previous one to use a data frame with any number of columns (here I'm just using 2) to parameterize the mapping.

### R Version
Dynamic dots are captured in the list `parms` within the function and column values are referenced as elements of that list.
```{r}
parms<-distinct(mtcars,cyl,gear)

list<-pmap(
  parms
  ,function (...){
    parms<-rlang::dots_list(...)
    mtcars %>%
      dplyr::filter(cyl==parms$cyl & gear==parms$gear) 
  }
)

df<-list_rbind(list)

print(df)

```

### Python Version
This is very similar to the R version above. The key is that each row of the data frame `parms` is turned into a dictionary (i.e., has the form `{'key1':_key1val_,...,'keyk':_keykval_}`). `iterator` is then a list of dictionaries (`iter_rows()` returns dictionaries when `named=True`) which allows me to capture a single row of all parameters needed in the body of the anonymous function with the single parameter `dctnry`. The parameter values can be referenced by the original `parms` data frame variable name in the function body via the dictionary method `get()`.
```{python}
mtcars=pl.read_csv("mtcars.csv")

parms=(
    mtcars
    .group_by("cyl","gear")
    .agg()
)

iterator=list(parms.iter_rows(named=True))

df=map(
    lambda dctnry: (
        mtcars
        .filter(
            (pl.col("cyl")==dctnry.get('cyl')) & 
            (pl.col("gear")==dctnry.get('gear'))
        )
    )
    ,iterator
)

df=pl.concat(list(df))

print(df)

```

## 9. Pivots
In this example I start with `mtcars` (a version with rownames mapped to the column `car`), pivot to a long file and then back to wide.

### R Version
```{r}
library(tidyr)
library(tibble)

cars<-rownames_to_column(mtcars,"car") %>%
  pivot_longer(
    cols=where(is.numeric)
    ,names_to="variable"
    ,values_to="value"
  )

print(cars)

mtcars<-cars %>%
  pivot_wider(
    id_cols=car
    ,names_from="variable"
    ,values_from="value"
  )

print(mtcars)

```

### Python Version
With polars, going from wide to long is an [unpivot](https://docs.pola.rs/api/python/dev/reference/dataframe/api/polars.DataFrame.unpivot.html) and long to wide is a [pivot](https://docs.pola.rs/api/python/dev/reference/dataframe/api/polars.DataFrame.pivot.html). Note that the lazy version of a pivot has a different structure in which it's preceeded by `collect()` and followed by `lazy()`---I've included an eager version for contrast.
```{python}
import polars.selectors as cs

mtcars=pl.scan_csv("mtcars_w_names.csv")

q=(
    mtcars
    .unpivot(
        on=cs.numeric()
        ,index="car"
        ,variable_name="variable"
        ,value_name="value"
    )
)

cars=q.collect()

print(cars)

#lazy
q=(
    cars.lazy()
    .collect()
    .pivot(
        index="car"
        ,on="variable"
        ,values="value"
        ,aggregate_function=None
    )
    .lazy()
)

mtcars=q.collect()

print(mtcars)

#eager
mtcars=(
    cars
    .pivot(
        index="car"
        ,on="variable"
        ,values="value"
        ,aggregate_function=None
    )
)

print(mtcars)

```