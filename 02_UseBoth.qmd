---
title: "Recreate Some dplyr Shiz with Python and Polars"
format: html
---

```{r}
#| echo: false
#| output: false

#library(tidyverse)
```


## 1. Basic Summarize without Generalization
### r version
```{r}
library(dplyr)

table<-mtcars %>%
    group_by(cyl) %>%
    summarize(mpg.mean=mean(mpg))

print(table)

```

### python version
I'm using polars instead of pandas as it feels more natural to me coming from tidy-style R programming. I'm invoking lazy evaluation (as opposed to eager) by using a lazy frame. This is actually more similar to dtplyr than dplyr. A query plan is assembled as `q` and then evaluated and the result saved to `table`. This 'lazy' approach allows polars to do query optimization.

While there is no pipe, the method chaining approach used below *feels* very familiar.
```{python}
import polars as pl

mtcars=pl.DataFrame(r.mtcars).lazy()

q=(
    mtcars
    .group_by(pl.col("cyl"))
    .agg(pl.col("mpg").mean().alias("mpg.mean"))
)

table=q.collect()

print(table)

```

## 2. Basic Mutate with Grouping and without Generalization
### r version
In R I can create a recode via `mutate()` that utilizes both group-level statistics and record-level data. This can be done in a single step with very little code.
```{r}
table<-mtcars %>%
    group_by(cyl) %>%
    mutate(rel.mpg=mpg/mean(mpg))

print(table)

```

### python version
With polars, it doesn't appear that I can mix the `group_by` and `with_columns` contexts. Equivalent output necessitates a 2-step approach that calculates and joins on the the group-level statistics before invoking my recode expression in the `with_columns` context, which is roughly analogous to `dplyr::mutate()`.

Note that I won't be surprised to learn that what I did above in the dplyr version *is* possible with polars (spoiler: it is--see update below) and I just don't yet know how. In any case, the 2-step works and still fits within a single method chain without too much additional typing.

Update: while `group_by` and `with_columns` contexts can't be mixed, grouping can be incorporated into the `with_columns` context via the window function `over()`. This is actually *more* powerful than `group_by() %>% mutate()` in R because you can have a different groupings across expressions in a common `with_columns` context.
```{python}
q=(
    mtcars.join(
        mtcars
        .group_by("cyl")
        .agg(pl.col("mpg").mean().alias("den"))
        ,on="cyl"
        ,how="left"
    )
    .with_columns((pl.col("mpg")/pl.col("den")).alias("rel.mpg"))
    .drop("den")
)

table=q.collect()

print(table)

#this produces equivalent output, while being computationally optimal and more concise
q=(
    mtcars
    .with_columns(
        ((pl.col("mpg"))/(pl.col("mpg").mean().over("cyl"))).alias("rel.mpg")
    )
)

table=q.collect()

print(table)

```

## 3. Summarize Generalized by Variable Type with Across()
### r version
```{r}
mtcars %>%
    group_by(cyl,gear) %>%
    summarize(
        across(
            .cols=where(is.double)
            ,.fns=mean
            ,.names="{.col}_mean"
        )
    )

```

### python version
Note that there are *many* selector functions available, as explained [here](https://docs.pola.rs/api/python/stable/reference/selectors.html). This is a good example of how a selector function (`cs.float()` in this case) works in conjunction with namaing methods, like `expr.name.suffix()` below. See other methods [here](https://docs.pola.rs/api/python/stable/reference/expressions/name.html).
```{python}
import polars.selectors as cs

q=(
    mtcars
    .group_by("cyl","gear")
    .agg(cs.float().mean().name.suffix("_mean"))
)

table=q.collect()

print(table)

```

## 4. Function for n & pct by Grouping Variables
In both cases I want a custom function to create simple, list-style frequency tables based on one or more variables provided by the user.

### r version
I use dynamic dots here to tunnel in the variables provided by the user in the function call. This is powerful and flexible, allowing for 0+ variables provided as naked symbols rather than strings (`doit()`); an alternative version (`doit2()`) also uses dynamic dots, but with the intention to call with variable names provided as strings--this scales up better and is more comparable to the python version.
```{r}
library(rlang)

doit<-function(df,...){
  df %>%
    ungroup() %>%
    mutate(N=n()) %>%
    group_by(...) %>%
    summarize(n=n(),pct=n()*100/mean(N),.groups="drop") %>%
    mutate(cumn=cumsum(n),cumpct=cumsum(pct))
}

doit(mtcars)
doit(mtcars,cyl)
doit(mtcars,cyl,gear)

doit2<-function(df,...){
    vars<-dots_list(...) %>%
        list_c() %>%
        syms()

    df %>%
        ungroup() %>%
        mutate(N=n()) %>%
        group_by(!!!vars) %>%
        summarize(n=n(),pct=n()*100/mean(N),.groups="drop") %>%
        mutate(cumn=cumsum(n),cumpct=cumsum(pct))
}

doit2(mtcars)
doit2(mtcars,"cyl")
doit2(mtcars,"cyl","gear")

```

### python version
The version below gets very close! The only differences are that the python version of `doit()` doesn't work as-is if 0 variables are provided (though it could be modifed to only conditionally invoke the `group_by` context) and the variable names are passed as strings (i.e., this doesn't seem to be optional as with the tidy versions). This latter point should actually simplify some situations that are complex due to data mask ambiguities in tidy functions.
```{python}
def doit(df,*argv):
    q=(
        df
        .with_columns(pl.len().alias("N"))
        .group_by(*argv)
        .agg(
            pl.len().alias("n")
            ,((pl.len()*100)/pl.col("N").mean()).alias("pct")
        )
        .sort(*argv)
        .with_columns(
            pl.col("n").cum_sum().alias("cumn")
            ,pl.col("pct").cum_sum().alias("cumpct")
        )
    )
    table=q.collect()
    print(table)

doit(mtcars,"cyl")
doit(mtcars,"cyl","gear")

```

## 5. Iterate a Custom Function
Here I want to apply the `doit` functions over parameters.

### r version
I use `purrr::pmap()` in the R version with a list of parameters. Since I defined the R version of `doit` to take naked symbols, the mapped version is kind of clunky--a cleaner alternative based on `doit2` highlights the point.
```{r}
parms<-list(
    list(mtcars,mtcars)
    ,"var1"=list(mtcars$cyl,mtcars$cyl)
    ,"var2"=list(mtcars$gear,mtcars$am)
)

purrr::pmap(parms,doit)

parms2<-list(
    c("cyl","cyl")
    ,c("gear","am")
)

purrr::pmap(parms2,doit2,df=mtcars)

```

### python version
Super simple! I combine 3 parameter series into a single iterator object via `zip`--can then map `doit` over `parms` via `itertools.starmap`. Awesome!
```{python}
import itertools as it

parms=zip(
    [mtcars,mtcars]
    ,['cyl','cyl']
    ,['gear','am']
)

list(it.starmap(doit,parms))

```

## 6. Conditional Recode

### r version
```{r}
mtcars %>%
    mutate(
        mpg.cat=case_when(
            mpg<10~"very bad"
            ,mpg<15~"bad"
            ,mpg<20~"okay"
            ,mpg<25~"good"
            ,TRUE~"great"
        )
    ) %>%
    arrange(desc(mpg))

```

### python version
This is clearly more wordy than the r version above. Note that `pl.lit()` seemed to be necessary for creating the recode as a string (maybe because the column used in the conditional is a float?).
```{python}
q=(
    mtcars
    .with_columns(
        pl.when(pl.col("mpg")<10).then(pl.lit("very bad"))
        .when(pl.col("mpg")<15).then(pl.lit("bad"))
        .when(pl.col("mpg")<20).then(pl.lit("okay"))
        .when(pl.col("mpg")<25).then(pl.lit("good"))
        .otherwise(pl.lit("great"))
        .alias("mpg.cat")
    )
    .sort("mpg",descending=True)
)

df=q.collect()

print(df)

```

## 7. Stack Data Frames by List Binding with Map and Anonymous Function
What I'm achieving with this example--returning `mtcars`--isn't very useful, but it illustrates something I do a lot: mapping an anonymous function over a vector to create a list of data frames which I subsequently stack together via row binding. In other words, in this example I'm reassembling `mtcars` by stacking together portions returned from each iteration of `map`.

### r version
Pretty straight forward. Note that parms is a vector here.
```{r}
parms<-distinct(mtcars,cyl) %>%
  pull()

list<-map(
  parms
  ,function (x){
    mtcars %>%
      dplyr::filter(cyl==x) %>%
      arrange(desc(mpg))
  }
)

df<-list_rbind(list)

print(df)

```

### python version
This is extremely similar. Note that I'm pulling from the csv of `mtcars` to utilize eager evaluation (for simplicity).
```{python}
mtcars=pl.read_csv("mtcars.csv")

parms=mtcars.get_column("cyl").unique()

df=map(
    lambda x: (
        mtcars
        .filter(pl.col("cyl")==x)
        .sort("mpg",descending=True)
    )
    ,parms
)

df=pl.concat(list(df))

print(df)

```

## 8. Stack Data Frames by List Binding with pmap and Anonymous Function of ...
This example generalizes the previous one to use a data frame with any number of columns (here I'm just using 2) to parameterize the mapping.

### r version
Dynamic dots are captured in the list `parms` within the function and column values are referenced as elements of that list.
```{r}
parms<-distinct(mtcars,cyl,gear)

list<-pmap(
  parms
  ,function (...){
    parms<-rlang::dots_list(...)
    mtcars %>%
      dplyr::filter(cyl==parms$cyl & gear==parms$gear) 
  }
)

df<-list_rbind(list)

print(df)

```

### python version
This is functionally very similar to the r version above. Note that I was originally thinking I would need `itertools.starmap()` here, but it proved unnecessary. The key is that `iterator` is a list of dictionaries (`iter_rows()` returns dictionaries when `named=True`) which allows me to capture a single dictionary with `x` for each iteration, and the variable values can be referenced by variable name in the function body as `x['name']`.
```{python}
mtcars=pl.read_csv("mtcars.csv")

parms=(
    mtcars
    .group_by("cyl","gear")
    .agg()
)

iterator=list(parms.iter_rows(named=True))

df=map(
    lambda x: (
        mtcars
        .filter(
            (pl.col("cyl")==x['cyl']) & (pl.col("gear")==x['gear'])
        )
    )
    ,iterator
)

df=pl.concat(list(df))

print(df)

```